{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serializing and Deserializing Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.admin import AdminClient, NewTopic, NewPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = [\"string_topic\", \"avro_topic\"]\n",
    "brokers = \"kafka1:9092,kafka2:9093,\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AdminClient(conf = {'bootstrap.servers': brokers})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_topics = [NewTopic(topic, num_partitions=2, \n",
    "        replication_factor=1) for topic in topics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = a.create_topics(new_topics)\n",
    "for topic, f in fs.items():\n",
    "    try:\n",
    "        f.result()  # The result itself is None\n",
    "        print(\"Topic {} created\".format(topic))\n",
    "    except Exception as e:\n",
    "        print(\"Failed to create topic {}: {}\".format(topic, e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import SerializingProducer\n",
    "from confluent_kafka.serialization import StringSerializer\n",
    "import json\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer_conf = {\n",
    "        'bootstrap.servers': brokers,\n",
    "        'key.serializer': StringSerializer('utf_8'),\n",
    "        'value.serializer': StringSerializer('utf_8')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer = SerializingProducer(producer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data.json') as f:\n",
    "    data = json.load(f)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in data:\n",
    "    print(m)\n",
    "    producer.produce(topic=topics[0],key=str(uuid4()),value=str(m))\n",
    "    producer.poll(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "producer.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consumer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import DeserializingConsumer\n",
    "from confluent_kafka.serialization import StringDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_deserializer = StringDeserializer('utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_conf = {\n",
    "    'bootstrap.servers': brokers,\n",
    "    'key.deserializer': string_deserializer,\n",
    "    'value.deserializer': string_deserializer,\n",
    "    'group.id': str(uuid4()),\n",
    "    'session.timeout.ms': 6000,\n",
    "    'auto.offset.reset': 'earliest'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer = DeserializingConsumer(consumer_conf)\n",
    "consumer.subscribe([topics[0]])\n",
    "topics[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    while True:\n",
    "        # SIGINT can't be handled when polling, limit timeout to 1 second.\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        if msg.error():\n",
    "            raise KafkaException(msg.error())\n",
    "        else:\n",
    "            print(\"{} [{}] at offset  {} with key  {}:  {}\".format(msg.topic(), msg.partition(), msg.offset(), str(msg.key()), str(msg.value())))\n",
    "except KeyboardInterrupt:\n",
    "    sys.stderr.write('%% Aborted by user\\n')\n",
    "finally:\n",
    "    # Close down consumer to commit final offsets.\n",
    "    consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka.schema_registry import SchemaRegistryClient\n",
    "from confluent_kafka.schema_registry.avro import AvroSerializer\n",
    "import random, time\n",
    "from uuid import uuid4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_registry_conf = {'url': \"http://schema-registry:8081\"}\n",
    "schema_registry_client = SchemaRegistryClient(schema_registry_conf)\n",
    "\n",
    "schema_registry_client.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_schema = open(\"observation.avsc\").read()\n",
    "value_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_serializer = AvroSerializer(value_schema, schema_registry_client)\n",
    "\n",
    "producer_conf = {'bootstrap.servers': brokers,\n",
    "                 'key.serializer': StringSerializer('utf_8'),\n",
    "                 'value.serializer': avro_serializer,\n",
    "                 'partitioner': 'murmur2_random',\n",
    "}\n",
    "\n",
    "producer = SerializingProducer(producer_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rooms = [\"room1\", \"room2\", \"room3\", \"room4\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,10):\n",
    "    value = {\"id\": i, \n",
    "               \"value\": random.uniform(0,50), \n",
    "               \"measurement\":\"temperature\", \n",
    "               \"timestamp\": round(time.time()*1000)}\n",
    "    key = rooms[ random.randint(0,3)]\n",
    "    print(key + \" \" + str(value))\n",
    "    producer.poll(0)\n",
    "    producer.produce(topic=topics[1], value=value, key=key)\n",
    "    producer.flush(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_registry_client.get_subjects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from confluent_kafka import DeserializingConsumer\n",
    "from confluent_kafka.schema_registry.avro import AvroDeserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avro_deserializer = AvroDeserializer(value_schema,schema_registry_client)\n",
    "\n",
    "string_deserializer = StringDeserializer('utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consumer_conf = {'bootstrap.servers': brokers,\n",
    "                     'key.deserializer': string_deserializer,\n",
    "                     'value.deserializer': avro_deserializer,\n",
    "                     'group.id': str(uuid4()),\n",
    "                     'auto.offset.reset': \"earliest\"}\n",
    "\n",
    "consumer = DeserializingConsumer(consumer_conf)\n",
    "consumer.subscribe([topics[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    try:\n",
    "        # SIGINT can't be handled when polling, limit timeout to 1 second.\n",
    "        msg = consumer.poll(1.0)\n",
    "        if msg is None:\n",
    "            continue\n",
    "        m = msg.value()\n",
    "        if m is not None:\n",
    "            print(m)\n",
    "    except KeyboardInterrupt:\n",
    "        break\n",
    "        consumer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What if we do not know the schema?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't see this in python because of a bug of the Avro library. However, we can observe it from [Java](link to the other repo class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
